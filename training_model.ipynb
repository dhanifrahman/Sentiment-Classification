{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### TRAINING MODEL ANALISIS SENTIMEN (NLP - DEEP LEARNING) ###\n",
        "# 1. Pendahuluan\n",
        "\n",
        "# Notebook ini digunakan khusus untuk pelatihan model analisis sentimen menggunakan dataset hasil scraping mandiri sebanyak 10.740 sampel data. Proyek ini mengklasifikasikan teks ke dalam 3 kelas sentimen: Negatif, Netral, dan Positif dengan pendekatan Deep Learning serta melakukan 3 skema pelatihan berbeda.\n",
        "\n",
        "\n",
        "# 2. Import Library\n",
        "\n",
        "!pip install Sastrawi\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "\n",
        "\n",
        "# 3. Load Dataset\n",
        "\n",
        "df = pd.read_csv('dataset_scraping.csv')\n",
        "print('Jumlah data:', df.shape[0])\n",
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "# 4. Preprocessing & Labeling Data\n",
        "\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "stopwords = StopWordRemoverFactory().get_stop_words()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = ' '.join([stemmer.stem(w) for w in text.split() if w not in stopwords])\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Kamus sentimen sederhana\n",
        "positive_words = [\n",
        "    'bagus','mantap','senang','puas','baik','cepat',\n",
        "    'recommended','suka','memuaskan'\n",
        "]\n",
        "\n",
        "negative_words = [\n",
        "    'jelek','buruk','kecewa','lambat','parah','error',\n",
        "    'tidak puas','lemot','gagal'\n",
        "]\n",
        "\n",
        "def label_sentiment(text):\n",
        "    score = 0\n",
        "    for word in text.split():\n",
        "        if word in positive_words:\n",
        "            score += 1\n",
        "        elif word in negative_words:\n",
        "            score -= 1\n",
        "\n",
        "    if score > 0:\n",
        "        return 'Positif'\n",
        "    elif score < 0:\n",
        "        return 'Negatif'\n",
        "    else:\n",
        "        return 'Netral'\n",
        "\n",
        "df['label'] = df['clean_text'].apply(label_sentiment)\n",
        "df['label'].value_counts()\n",
        "\n",
        "# 5. Label Encoding\n",
        "\n",
        "label_map = {'Negatif':0, 'Netral':1, 'Positif':2}\n",
        "df['label_enc'] = df['label'].map(label_map)\n",
        "\n",
        "\n",
        "\n",
        "# 6. Tokenisasi dan Padding\n",
        "\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(df['clean_text'])\n",
        "\n",
        "X = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "y = tf.keras.utils.to_categorical(df['label_enc'], num_classes=3)\n",
        "\n",
        "\n",
        "\n",
        "# 7. SKEMA 1 – LSTM (Split 80:20)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model1 = Sequential([\n",
        "    Embedding(20000, 128),\n",
        "    LSTM(128),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model1.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history1 = model1.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 8. SKEMA 2 – BiLSTM (Split 80:20)\n",
        "\n",
        "model2 = Sequential([\n",
        "    Embedding(20000, 128),\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model2.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history2 = model2.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 9. SKEMA 3 – CNN (Split 70:30)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model3 = Sequential([\n",
        "    Embedding(20000, 128),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model3.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history3 = model3.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 10. Evaluasi Model Terbaik\n",
        "\n",
        "y_pred = model2.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "\n",
        "\n",
        "# 11. Inference / Testing\n",
        "\n",
        "sample_text = ['produk ini sangat bagus dan pengirimannya cepat']\n",
        "seq = tokenizer.texts_to_sequences(sample_text)\n",
        "pad = pad_sequences(seq, maxlen=100)\n",
        "pred = model2.predict(pad)\n",
        "\n",
        "label = ['Negatif','Netral','Positif']\n",
        "print('Prediksi Sentimen:', label[np.argmax(pred)])\n",
        "\n",
        "\n",
        "\n",
        "# 12. Kesimpulan\n",
        "\n",
        "# Model deep learning berbasis LSTM, BiLSTM, dan CNN berhasil melakukan klasifikasi sentimen dengan sangat baik pada dataset hasil scraping mandiri sebanyak 10.740 data, dengan akurasi testing mencapai hingga hampir 100%, serta memenuhi seluruh kriteria proyek analisis sentimen NLP."
      ],
      "metadata": {
        "id": "TxkylqlkNMpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc4f7d3-7d54-42db-bd21-391690473439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Jumlah data: 10740\n",
            "Epoch 1/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.7111 - loss: 0.7804 - val_accuracy: 0.8901 - val_loss: 0.2917\n",
            "Epoch 2/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9583 - loss: 0.1322 - val_accuracy: 0.9837 - val_loss: 0.0655\n",
            "Epoch 3/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9851 - loss: 0.0471 - val_accuracy: 0.9832 - val_loss: 0.0427\n",
            "Epoch 4/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9902 - loss: 0.0373 - val_accuracy: 0.9874 - val_loss: 0.0372\n",
            "Epoch 5/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9899 - loss: 0.0273 - val_accuracy: 0.9930 - val_loss: 0.0207\n",
            "Epoch 6/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9986 - loss: 0.0073 - val_accuracy: 0.9953 - val_loss: 0.0186\n",
            "Epoch 7/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9987 - loss: 0.0087 - val_accuracy: 0.9949 - val_loss: 0.0165\n",
            "Epoch 8/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9981 - loss: 0.0123 - val_accuracy: 0.9967 - val_loss: 0.0164\n",
            "Epoch 9/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9996 - loss: 0.0029 - val_accuracy: 0.9953 - val_loss: 0.0180\n",
            "Epoch 10/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 0.0021 - val_accuracy: 0.9963 - val_loss: 0.0164\n",
            "Epoch 11/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9967 - val_loss: 0.0181\n",
            "Epoch 1/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.7036 - loss: 0.7692 - val_accuracy: 0.9744 - val_loss: 0.1538\n",
            "Epoch 2/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9824 - loss: 0.0888 - val_accuracy: 0.9842 - val_loss: 0.0420\n",
            "Epoch 3/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9877 - loss: 0.0380 - val_accuracy: 0.9912 - val_loss: 0.0246\n",
            "Epoch 4/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9983 - loss: 0.0131 - val_accuracy: 0.9935 - val_loss: 0.0206\n",
            "Epoch 5/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9959 - loss: 0.0146 - val_accuracy: 0.9939 - val_loss: 0.0136\n",
            "Epoch 6/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9985 - loss: 0.0073 - val_accuracy: 0.9939 - val_loss: 0.0192\n",
            "Epoch 7/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0063 - val_accuracy: 0.9939 - val_loss: 0.0197\n",
            "Epoch 8/15\n",
            "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9966 - loss: 0.0099 - val_accuracy: 0.9949 - val_loss: 0.0175\n",
            "Epoch 1/15\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6787 - loss: 0.8133 - val_accuracy: 0.9733 - val_loss: 0.1400\n",
            "Epoch 2/15\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9751 - loss: 0.1174 - val_accuracy: 0.9839 - val_loss: 0.0443\n",
            "Epoch 3/15\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9860 - loss: 0.0502 - val_accuracy: 0.9953 - val_loss: 0.0231\n",
            "Epoch 4/15\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9896 - loss: 0.0385 - val_accuracy: 0.9941 - val_loss: 0.0187\n",
            "Epoch 5/15\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9932 - loss: 0.0264 - val_accuracy: 0.9950 - val_loss: 0.0172\n",
            "Epoch 6/15\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9909 - loss: 0.0309 - val_accuracy: 0.9950 - val_loss: 0.0186\n",
            "Epoch 7/15\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9964 - loss: 0.0140 - val_accuracy: 0.9947 - val_loss: 0.0184\n",
            "Epoch 8/15\n",
            "\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9959 - loss: 0.0206 - val_accuracy: 0.9947 - val_loss: 0.0187\n",
            "\u001b[1m101/101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99       389\n",
            "           1       1.00      1.00      1.00      2260\n",
            "           2       1.00      0.99      0.99       573\n",
            "\n",
            "    accuracy                           1.00      3222\n",
            "   macro avg       0.99      0.99      0.99      3222\n",
            "weighted avg       1.00      1.00      1.00      3222\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "Prediksi Sentimen: Positif\n"
          ]
        }
      ]
    }
  ]
}